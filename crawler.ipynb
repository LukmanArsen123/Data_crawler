{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a657b17-c339-4c55-9dd3-b699bcfa80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\arsen\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\arsen\\anaconda3\\lib\\site-packages (0.3.29)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\arsen\\anaconda3\\lib\\site-packages (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters in c:\\users\\arsen\\anaconda3\\lib\\site-packages (0.3.11)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\arsen\\anaconda3\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\arsen\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Collecting dashscope\n",
      "  Downloading dashscope-1.24.6-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\arsen\\anaconda3\\lib\\site-packages (5.2.1)\n",
      "Collecting html2text\n",
      "  Downloading html2text-2025.4.15-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain) (0.4.29)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\arsen\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core) (4.13.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core) (24.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from dashscope) (1.8.0)\n",
      "Requirement already satisfied: cryptography in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from dashscope) (43.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from cryptography->dashscope) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography->dashscope) (2.21)\n",
      "Requirement already satisfied: anyio in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.0.0)\n",
      "Downloading dashscope-1.24.6-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 182.3 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 182.3 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 182.3 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 182.3 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 182.3 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 182.3 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 182.3 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 0.8/1.3 MB 191.7 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 0.8/1.3 MB 191.7 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 0.8/1.3 MB 191.7 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 1.0/1.3 MB 223.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.0/1.3 MB 223.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.0/1.3 MB 223.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.0/1.3 MB 223.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.0/1.3 MB 223.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.0/1.3 MB 223.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.0/1.3 MB 223.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.0/1.3 MB 223.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.0/1.3 MB 223.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.0/1.3 MB 223.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.0/1.3 MB 223.7 kB/s eta 0:00:02\n",
      "   ---------------------------------------- 1.3/1.3 MB 183.7 kB/s eta 0:00:00\n",
      "Downloading html2text-2025.4.15-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: html2text, dashscope\n",
      "Successfully installed dashscope-1.24.6 html2text-2025.4.15\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-core langchain-text-splitters beautifulsoup4 requests python-dotenv dashscope lxml html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7607ccf-a6b5-42bf-8ebe-34b01b096faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import tldextract\n",
    "from typing import List, Dict, Optional, Any, Set\n",
    "from bs4 import BeautifulSoup\n",
    "from dataclasses import dataclass, field\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "OPENROUTER_API_KEY = \"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\"\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "QWEN_MODEL = \"qwen/qwen-3-4b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159567d6-8f30-4837-9886-23630e25c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from pydantic import BaseModel\n",
    "import requests\n",
    "\n",
    "class QwenLLM(LLM, BaseModel):\n",
    "    api_key: str = OPENROUTER_API_KEY\n",
    "    api_url: str = f\"{OPENROUTER_BASE_URL}/chat/completions\"\n",
    "    model: str = QWEN_MODEL\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"HTTP-Referer\": \"http://localhost\",\n",
    "            \"X-Title\": \"IntelligentCrawler\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        resp = requests.post(self.api_url, headers=headers, json=payload, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        j = resp.json()\n",
    "        return j[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    async def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        return self._call(prompt, stop)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"model\": self.model}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"qwen\"\n",
    "\n",
    "llm = QwenLLM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f77b4428-f3a0-4281-a1c7-5c6d3f3b2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CrawlerAgent:\n",
    "    session: aiohttp.ClientSession\n",
    "    max_pages: int = 100\n",
    "    max_depth: int = 2\n",
    "    delay: float = 0.5\n",
    "    visited: Set[str] = field(default_factory=set)\n",
    "    domain_limit: Optional[str] = None\n",
    "\n",
    "    async def fetch(self, url: str) -> Optional[str]:\n",
    "        try:\n",
    "            async with self.session.get(url, timeout=20) as resp:\n",
    "                if resp.status == 200 and 'text' in resp.headers.get('content-type',''):\n",
    "                    text = await resp.text(errors='ignore')\n",
    "                    await asyncio.sleep(self.delay)\n",
    "                    return text\n",
    "        except Exception as e:\n",
    "            print(f\"[fetch error] {url} -> {e}\")\n",
    "        return None\n",
    "\n",
    "    def extract_links_and_text(self, base_url: str, html: str) -> Dict[str, Any]:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        texts = [t.get_text(\" \", strip=True) for t in soup.find_all(['p','h1','h2','h3','li'])]\n",
    "        links = set()\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a['href'].strip()\n",
    "            if href.startswith((\"mailto:\", \"javascript:\")):\n",
    "                continue\n",
    "            full = urljoin(base_url, href)\n",
    "            parsed = urlparse(full)\n",
    "            if parsed.scheme in (\"http\",\"https\"):\n",
    "                links.add(full.split('#')[0])\n",
    "        return {\"texts\": texts, \"links\": list(links)}\n",
    "\n",
    "    async def crawl(self, seed_url: str):\n",
    "        to_visit = [(seed_url, 0)]\n",
    "        self.domain_limit = tldextract.extract(seed_url).registered_domain\n",
    "        results = []\n",
    "        while to_visit and len(self.visited) < self.max_pages:\n",
    "            url, depth = to_visit.pop(0)\n",
    "            if url in self.visited or depth > self.max_depth:\n",
    "                continue\n",
    "            if self.domain_limit and tldextract.extract(url).registered_domain != self.domain_limit:\n",
    "                continue\n",
    "            html = await self.fetch(url)\n",
    "            self.visited.add(url)\n",
    "            if not html:\n",
    "                continue\n",
    "            parsed = self.extract_links_and_text(url, html)\n",
    "            results.append({\"url\": url, \"text_blocks\": parsed[\"texts\"], \"links\": parsed[\"links\"]})\n",
    "            for link in parsed[\"links\"]:\n",
    "                if link not in self.visited and len(self.visited)+len(to_visit) < self.max_pages:\n",
    "                    to_visit.append((link, depth+1))\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f848f89-dd70-4117-b4ee-d9406a3d4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CleaningAgent:\n",
    "    llm: QwenLLM\n",
    "    stop_words: set = field(default_factory=lambda: set(stopwords.words('english')))\n",
    "\n",
    "    def basic_clean_text(self, text: str) -> str:\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def merge_blocks(self, blocks: List[str]) -> str:\n",
    "        seen, out = set(), []\n",
    "        for b in blocks:\n",
    "            b_clean = b.strip()\n",
    "            if b_clean and b_clean.lower() not in seen:\n",
    "                seen.add(b_clean.lower())\n",
    "                out.append(b_clean)\n",
    "        return \"\\n\\n\".join(out)\n",
    "\n",
    "    def evaluate_quality(self, text: str) -> Dict[str, Any]:\n",
    "        words = word_tokenize(text)\n",
    "        return {\n",
    "            \"num_words\": len(words),\n",
    "            \"avg_word_len\": sum(len(w) for w in words)/max(1,len(words)),\n",
    "            \"stop_words_pct\": sum(1 for w in words if w.lower() in self.stop_words)/max(1,len(words))\n",
    "        }\n",
    "\n",
    "    def clean_and_summarize(self, blocks: List[str]) -> Dict[str, Any]:\n",
    "        merged = self.merge_blocks([self.basic_clean_text(b) for b in blocks])\n",
    "        quality = self.evaluate_quality(merged)\n",
    "        try:\n",
    "            cleaned = self.llm._call(\n",
    "                f\"Clean this web text. Remove navigation/boilerplate, keep only meaningful content:\\n{merged}\"\n",
    "            )\n",
    "        except:\n",
    "            cleaned = merged\n",
    "        try:\n",
    "            summary = self.llm._call(\n",
    "                f\"Summarize this text in 3-6 sentences and list 5 key points:\\n{cleaned}\"\n",
    "            )\n",
    "        except:\n",
    "            summary = cleaned[:1000]\n",
    "        return {\"cleaned_text\": cleaned, \"summary\": summary, \"quality\": quality}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58fb8fa5-7a6c-45ac-8e24-b587377617b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_pipeline(user_input: str,\n",
    "                       max_pages: int = 30,\n",
    "                       max_depth: int = 2,\n",
    "                       delay: float = 0.5):\n",
    "    is_url = user_input.startswith((\"http://\", \"https://\"))\n",
    "    seed_urls = [user_input] if is_url else []\n",
    "\n",
    "    if not is_url:\n",
    "        try:\n",
    "            resp = llm._call(\n",
    "                f\"Suggest up to 5 reliable URLs to learn about: {user_input}. Return only JSON array.\"\n",
    "            )\n",
    "            seed_urls = re.findall(r\"https?://[^\\s,\\\"\\]]+\", resp)[:5]\n",
    "        except:\n",
    "            return {\"error\":\"no_seed_urls\"}\n",
    "\n",
    "    async with aiohttp.ClientSession(headers={\"User-Agent\":\"IntelligentCrawler/1.0\"}) as session:\n",
    "        crawler = CrawlerAgent(session, max_pages, max_depth, delay)\n",
    "        all_pages, visited = [], set()\n",
    "        for seed in seed_urls:\n",
    "            pages = await crawler.crawl(seed)\n",
    "            all_pages.extend(pages)\n",
    "            if len(crawler.visited) >= max_pages: break\n",
    "\n",
    "    cleaner = CleaningAgent(llm=llm)\n",
    "    cleaned_results = [\n",
    "        {\"url\": p[\"url\"], **cleaner.clean_and_summarize(p.get(\"text_blocks\", []))}\n",
    "        for p in all_pages\n",
    "    ]\n",
    "    return {\"seed_urls\": seed_urls, \"raw_pages\": all_pages, \"cleaned\": cleaned_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36604576-ca5d-4fe1-b915-08556e9cf39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sync(user_input: str, **kwargs):\n",
    "    return asyncio.run(run_pipeline(user_input, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87c15795-ad17-47b6-a95c-01231842b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in c:\\users\\arsen\\anaconda3\\lib\\site-packages (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c1f5f31-f760-4f3e-a501-5d4b3c13e30e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsen\\AppData\\Local\\Temp\\ipykernel_27356\\3232067358.py:38: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
      "  self.domain_limit = tldextract.extract(seed_url).registered_domain\n",
      "C:\\Users\\arsen\\AppData\\Local\\Temp\\ipykernel_27356\\3232067358.py:44: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
      "  if self.domain_limit and tldextract.extract(url).registered_domain != self.domain_limit:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped pages: 4\n",
      "Saved crawler_cleaned_results.csv\n"
     ]
    }
   ],
   "source": [
    "seed = \"https://en.wikipedia.org/wiki/Heliocentrism\"  \n",
    "out = run_sync(seed, max_pages=5, max_depth=1, delay=0.3)\n",
    "\n",
    "print(\"Scraped pages:\", len(out.get(\"raw_pages\", [])))\n",
    "pd.DataFrame([\n",
    "    {\"url\":c[\"url\"], \"summary\": c[\"summary\"], \"cleaned\": c[\"cleaned_text\"]}\n",
    "    for c in out[\"cleaned\"]\n",
    "]).to_csv(\"crawler_cleaned_results.csv\", index=False)\n",
    "print(\"Saved crawler_cleaned_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de478ea6-9208-43a5-ae5d-a6c04e194c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arsen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\arsen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arsen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4880a-b21f-4be0-b71e-0063b7252229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
